lora_config:
    name: lima-st
    r: 256
    lora_alpha: 512
    target_modules:
        [
            'v_proj',
            'o_proj',
            'down_proj',
            'gate_proj',
            'up_proj',
            'q_proj',
            'k_proj'
        ]
data_config:
    max_length: 2048
    template: 'alpaca_input'
    input_column: "input"
    output_column: "output"
    test_split: 0.1
training_config:
    epochs: 2
    warmup_ratio: 0.3
    batch_size: 8
    grad_accum_steps: 8
