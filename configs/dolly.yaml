lora_config:
    name: dolly-15k
    r: 256
    lora_alpha: 512
    target_modules:
        [
            'v_proj',
            'o_proj',
            'down_proj',
            'gate_proj',
            'up_proj',
            'q_proj',
            'k_proj'
        ]
data_config:
    max_length: 2048
    template: 'alpaca_input'
    instruction_column: 'instruction'
    input_column: 'context'
    output_column: 'response'
    test_split: 0.15
training_config:
    epochs: 10
    steps: 500
    warmup_ratio: 0.3
    batch_size: 12
    grad_accum_steps: 8
